{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQzvamOuNZwj",
        "outputId": "a2e2734d-bc8a-4d48-cc74-cb5c716faf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 [==============================] - 33s 108ms/step - loss: 0.5806 - accuracy: 0.7866 - val_loss: 0.4707 - val_accuracy: 0.8142\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 32s 108ms/step - loss: 0.3606 - accuracy: 0.8676 - val_loss: 0.3498 - val_accuracy: 0.8749\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 31s 104ms/step - loss: 0.3087 - accuracy: 0.8864 - val_loss: 0.3080 - val_accuracy: 0.8872\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 32s 106ms/step - loss: 0.2748 - accuracy: 0.8978 - val_loss: 0.2897 - val_accuracy: 0.8911\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 32s 105ms/step - loss: 0.2497 - accuracy: 0.9078 - val_loss: 0.2766 - val_accuracy: 0.8992\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 31s 104ms/step - loss: 0.2323 - accuracy: 0.9138 - val_loss: 0.2596 - val_accuracy: 0.9051\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 32s 108ms/step - loss: 0.2166 - accuracy: 0.9191 - val_loss: 0.2657 - val_accuracy: 0.9019\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 31s 104ms/step - loss: 0.2004 - accuracy: 0.9263 - val_loss: 0.2560 - val_accuracy: 0.9056\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 32s 105ms/step - loss: 0.1865 - accuracy: 0.9308 - val_loss: 0.2442 - val_accuracy: 0.9114\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 32s 106ms/step - loss: 0.1738 - accuracy: 0.9352 - val_loss: 0.2626 - val_accuracy: 0.9050\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# One hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Additional layer for key generation\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"mnist_cnn_model.h5\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = load_model(\"mnist_cnn_model.h5\")\n",
        "\n",
        "# Create a new model that shares all layers up to and including the new dense layer with the original model\n",
        "key_gen_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "def generate_key(image):\n",
        "    # Make sure the image is normalized and in the right shape\n",
        "    image = image / 255\n",
        "    image = image.reshape((1, 28, 28, 1))\n",
        "\n",
        "    # Generate the key\n",
        "    key = key_gen_model.predict(image)\n",
        "\n",
        "    # Scale the key's elements to be between 0 and 255\n",
        "    key = (key - key.min()) / (key.max() - key.min()) * 255\n",
        "\n",
        "    # Round the key's elements to the nearest integers\n",
        "    key = np.round(key)\n",
        "\n",
        "    # Reshape the key to a 1D array\n",
        "    key = key.flatten()\n",
        "\n",
        "    # Truncate the key to be the length of row*col of the image\n",
        "    key = key[:28*28]\n",
        "\n",
        "    # # Reshape the key back to a 2D array\n",
        "    # key = key.reshape((28, 28))\n",
        "\n",
        "    return key\n",
        "\n",
        "# Test the function with a sample image\n",
        "sample_image = X_train[0]\n",
        "key = generate_key(sample_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNtlXfmYUopL",
        "outputId": "2952d88c-9de1-4522-c028-c58ac54c2ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f819b23f5b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 66ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key"
      ],
      "metadata": {
        "id": "SmzF2LScW6Eo",
        "outputId": "e187f82b-0cca-4409-f416-650c67f4ab34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([163., 217., 144.,   0.,  88., 112.,  24., 166., 230., 163., 159.,\n",
              "       123., 172.,  66.,  40.,  76., 161.,  44.,  72.,   1., 141., 146.,\n",
              "       152., 215., 237., 185., 149., 112., 171.,  92., 112., 103., 114.,\n",
              "       105., 162., 154.,  66.,  63.,  90., 186., 190., 227., 194., 125.,\n",
              "       114., 160., 144.,  70., 125., 189.,  62.,  89., 115., 188., 177.,\n",
              "       165.,  39., 192., 171., 157., 149., 137.,  57., 139.,  72., 204.,\n",
              "        31., 186.,  82., 193., 175.,  32.,  54., 115., 143., 111., 150.,\n",
              "       233., 102.,  97.,  81.,  79., 226., 168.,  70., 138.,  39., 137.,\n",
              "        48., 207., 144., 162.,  89., 107.,  86., 158., 222., 138.,  95.,\n",
              "       178., 145., 148., 179.,  92.,  54.,  37., 224.,  96., 107., 250.,\n",
              "       223., 191., 240., 139.,  82.,  36., 204., 215.,  82.,  55., 134.,\n",
              "       165., 255., 134.,  52.,  67.,  97., 140.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUTLBcWXDvUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}